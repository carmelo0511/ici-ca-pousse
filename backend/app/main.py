from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from typing import Dict, List, Optional
import logging
from contextlib import asynccontextmanager

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Variables globales pour les services ML (seront initialis√©s)
ml_pipeline = None
ensemble_model = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application"""
    global ml_pipeline, ensemble_model
    try:
        # Import des services ML
        logger.info("Initialisation des services ML...")
        from services.ml_pipeline import MLPipeline
        from models.ensemble_model import AdvancedEnsembleModel
        
        ml_pipeline = MLPipeline()
        ensemble_model = AdvancedEnsembleModel()
        logger.info("‚úÖ Services ML initialis√©s avec succ√®s")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'initialisation des services ML: {e}")
        logger.info("üîÑ Mode fallback activ√©")
    
    yield  # L'application s'ex√©cute ici
    
    # Nettoyage lors de l'arr√™t
    logger.info("Arr√™t de l'application")

app = FastAPI(
    title="Ici √áa Pousse ML API", 
    version="2.0.0", 
    description="API ML avanc√©e pour la pr√©diction de poids en musculation",
    lifespan=lifespan
)

# CORS pour React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "https://ici-ca-pousse.vercel.app", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic pour les requ√™tes
class PredictionRequest(BaseModel):
    exercise_name: str
    user_data: Dict
    workout_history: List[Dict]

class TrainingRequest(BaseModel):
    user_id: str
    new_data: List[Dict]
    retrain: bool = False

class AnalyticsResponse(BaseModel):
    model_performance: Dict
    feature_importance: Dict
    training_history: Dict
    prediction_accuracy: Dict

@app.get("/health")
async def health_check():
    """Health check pour Docker et monitoring"""
    return {
        "status": "healthy", 
        "service": "ici-ca-pousse-ml-api",
        "ml_services": {
            "pipeline": ml_pipeline is not None,
            "ensemble": ensemble_model is not None
        }
    }

@app.post("/api/ml/predict")
async def predict_weight(request: PredictionRequest):
    """Pr√©diction de poids avec pipeline ML avanc√©"""
    try:
        if ml_pipeline is None:
            # Fallback vers pr√©diction simple
            return await simple_prediction_fallback(request)
        
        prediction = await ml_pipeline.predict(
            exercise_name=request.exercise_name,
            user_data=request.user_data,
            workout_history=request.workout_history
        )
        
        return {
            "success": True,
            "prediction": prediction,
            "model_info": ml_pipeline.get_model_info() if hasattr(ml_pipeline, 'get_model_info') else {},
            "confidence": prediction.get("confidence", 0.5)
        }
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction: {e}")
        # Fallback vers pr√©diction simple en cas d'erreur
        return await simple_prediction_fallback(request)

@app.post("/api/ml/train")
async def train_models(request: TrainingRequest):
    """Entra√Ænement des mod√®les avec nouvelles donn√©es"""
    try:
        if ml_pipeline is None:
            raise HTTPException(status_code=503, detail="Service ML non disponible")
        
        training_result = await ml_pipeline.train(
            user_id=request.user_id,
            new_data=request.new_data,
            retrain=request.retrain
        )
        
        return {
            "success": True,
            "training_result": training_result,
            "model_performance": ml_pipeline.get_performance_metrics() if hasattr(ml_pipeline, 'get_performance_metrics') else {}
        }
    except Exception as e:
        logger.error(f"Erreur lors de l'entra√Ænement: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ml/analytics")
async def get_analytics():
    """M√©triques et analytics du pipeline ML"""
    try:
        if ml_pipeline is None:
            return {
                "error": "Service ML non disponible",
                "fallback_available": True
            }
        
        return {
            "model_performance": ml_pipeline.get_performance_metrics() if hasattr(ml_pipeline, 'get_performance_metrics') else {},
            "feature_importance": ml_pipeline.get_feature_importance() if hasattr(ml_pipeline, 'get_feature_importance') else {},
            "training_history": ml_pipeline.get_training_history() if hasattr(ml_pipeline, 'get_training_history') else {},
            "prediction_accuracy": ml_pipeline.get_prediction_accuracy() if hasattr(ml_pipeline, 'get_prediction_accuracy') else {}
        }
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def simple_prediction_fallback(request: PredictionRequest):
    """Pr√©diction de fallback simple sans ML complexe"""
    try:
        # Logique de pr√©diction simple
        current_weight = request.user_data.get('current_weight', 0)
        
        # Analyse simple de l'historique
        if not request.workout_history:
            increment = 2.5  # Incr√©ment par d√©faut
        else:
            # Calculer la progression moyenne des derni√®res s√©ances
            weights = []
            for workout in request.workout_history[-5:]:  # 5 derni√®res s√©ances
                for exercise in workout.get('exercises', []):
                    if exercise.get('name') == request.exercise_name:
                        for set_data in exercise.get('sets', []):
                            if set_data.get('weight'):
                                weights.append(float(set_data['weight']))
            
            if len(weights) >= 2:
                # Progression moyenne
                progression = (weights[-1] - weights[0]) / max(1, len(weights) - 1)
                increment = max(0.5, min(5.0, progression + 1.0))
            else:
                increment = 2.5
        
        predicted_weight = current_weight + increment
        
        return {
            "success": True,
            "prediction": {
                "exercise_name": request.exercise_name,
                "predicted_weight": round(predicted_weight, 1),
                "confidence": 0.6,
                "model_used": "simple_fallback",
                "recommendations": [
                    f"Poids recommand√©: {predicted_weight:.1f}kg",
                    "Pr√©diction bas√©e sur l'algorithme de fallback"
                ]
            },
            "model_info": {
                "type": "fallback",
                "description": "Algorithme de pr√©diction simple"
            },
            "confidence": 0.6
        }
    except Exception as e:
        logger.error(f"Erreur dans le fallback: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction: {str(e)}")

@app.get("/api/ml/status")
async def get_ml_status():
    """Statut des services ML"""
    return {
        "ml_pipeline_available": ml_pipeline is not None,
        "ensemble_model_available": ensemble_model is not None,
        "fallback_mode": ml_pipeline is None,
        "version": "2.0.0",
        "features": {
            "prediction": True,
            "training": ml_pipeline is not None,
            "analytics": ml_pipeline is not None,
            "fallback": True
        }
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)